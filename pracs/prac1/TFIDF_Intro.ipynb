{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
      ],
      "metadata": {
        "id": "r3Kg-M1EMZCZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TF-IDF\n",
        "$TF$ - частота встречания терма (term frequency)<br>\n",
        "$IDF$ - обратная частота встречания терма в документах (invert document frequency)\n",
        "\n",
        "$$tf_{i, j} = \\frac{N_i}{\\sum\\limits_j N_j}$$<br>\n",
        "(1) $$idf_{i} = log(\\frac{|D|}{|\\{d_k \\in D | i \\in d_k \\}|}) \\approx log (\\frac{|D|}{df(i) + 1}) \\approx log (\\frac{|D| + 1}{df(i) + 1}) + 1$$\n",
        "\n",
        "(2) $$idf_{i} = log(\\frac{|D|}{|\\{d_k \\in D | i \\in d_k \\}|}) + 1$$\n"
      ],
      "metadata": {
        "id": "hVJIQn7LPcxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Это ознакомительное задание. В нём от вас требуется\n",
        "    \n",
        "* Задать правильную формулу TF.IDF в функции `__calc_token_weight`,\n",
        "* Передать в неё правильные параметры.\n",
        "\n",
        "_NB! В этой задаче в словарях с частотами бывает удобно использовать_ `defaultdict` _или_ `Counter`"
      ],
      "metadata": {
        "id": "DK0NpKzzMOTT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9BZ-1skaL-bS"
      },
      "outputs": [],
      "source": [
        "class TfIdfExtractor:\n",
        "    def __init__(self, ngrams=1):\n",
        "        self.token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
        "        self.ngrams = ngrams\n",
        "\n",
        "        self.docs_with_token, self.token_hits_count_in_doc, self.token_count_in_doc = None, None, None\n",
        "        self.docs_count = None\n",
        "\n",
        "    def compute_counts_in_collection_collection(self, documents):\n",
        "        \"\"\"\n",
        "        Заполняем словари (хэшмэпы, ассоциативные массивы) разных частот для последующего вычисления tf-idf\n",
        "        :param documents: сырые тексты\n",
        "        \"\"\"\n",
        "        self.docs_with_token, self.token_hits_count_in_doc, self.token_count_in_doc = self.__count_tokens(documents)\n",
        "        self.docs_count = len(documents)\n",
        "\n",
        "    def __count_tokens(self, documents):\n",
        "        \"\"\"\n",
        "        Реализуйте метод, который строит словари (хэшмэпы, ассоциативные массивы) разных частот для последующего вычисления tf-idf.\n",
        "        Здесь не требуется никаких трюков, тренируемся считать явно.\n",
        "\n",
        "        :param documents: сырые тексты\n",
        "        :return: кортеж-тройка:\n",
        "        docs_with_token,            # словарь: docs_with_token[токен] = число содержащих его документов\n",
        "        token_hits_count_in_doc,    # словарь: token_hits_count_in_doc[документ][токен] = сколько раз токен встратился в данном документе\n",
        "        token_count_in_doc          # словарь: token_count_in_doc[документ] = сколько всего токенов в документе\n",
        "        \"\"\"\n",
        "\n",
        "        docs_with_token = ...\n",
        "        token_hits_count_in_doc = ...\n",
        "        token_count_in_doc = ...\n",
        "\n",
        "        for doc in documents:\n",
        "\n",
        "            # частоты токенов в этом документе\n",
        "            token_dictionary = ...\n",
        "            token_count = 0\n",
        "\n",
        "            for token in self.__get_ngrams(doc):\n",
        "                token = token.lower()\n",
        "\n",
        "                # считаем частоты отдельных токенов\n",
        "                # YOUR CODE HERE\n",
        "\n",
        "            token_hits_count_in_doc.append(token_dictionary)\n",
        "            token_count_in_doc.append(token_count)\n",
        "\n",
        "            # для каждого токена ведём учёт числа документов, в которых он встретился\n",
        "            # YOUR CODE HERE\n",
        "\n",
        "        return docs_with_token, token_hits_count_in_doc, token_count_in_doc\n",
        "\n",
        "    def __get_ngrams(self, doc):\n",
        "        \"\"\"\n",
        "        Функция, вычисляющая tf-idf, для которого всё подготовлено.\n",
        "        При необходимости используйте функцию findall() у поля self.token_pattern и английские стоп-слова.\n",
        "\n",
        "        :param doc: документ\n",
        "        :return: tokens список, состоящий из n-грам\n",
        "        \"\"\"\n",
        "        \n",
        "        tokens = []\n",
        "        # YOUR CODE HERE\n",
        "        # здесь вам может пригодиться структура данных deque\n",
        "        return tokens\n",
        "\n",
        "    def __calc_token_weight(self, document_count, docs_with_token, tokens_in_doc, tokens_total):\n",
        "        \"\"\"\n",
        "        Метод, вычисляющий tf-idf, для которого всё подготовлено.\n",
        "        При необходимости используйте math.log.\n",
        "\n",
        "        :param document_count: общее число документов\n",
        "        :param docs_with_token: число документов\n",
        "        :param tokens_in_doc: сколько раз токен встретился в документе\n",
        "        :param tokens_total: общее число токенов в данном документе\n",
        "        :return: tf.idf\n",
        "        \"\"\"\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        return 1.0\n",
        "\n",
        "    def extract_tfidf(self, topn_docs):\n",
        "        \"\"\"\n",
        "        Считаем топ по tf-idf для первых N документов.\n",
        "\n",
        "        :param topn_docs: число документов из списка\n",
        "        :return: extracted_tfidf список пар <токен, tf-idf>, отсортированных по найденным tf-idf\n",
        "        \"\"\"\n",
        "\n",
        "        extracted_tfidf = []\n",
        "        for n_doc in range(topn_docs):\n",
        "            token_weights = {}\n",
        "\n",
        "            for token in token_hits_count_in_doc[n_doc]:\n",
        "                token_weights[token] = self.__calc_token_weight(self.docs_count,\n",
        "                                                          # число документов, содержащих token\n",
        "                                                          ...,\n",
        "                                                          # сколько раз встретился токен token в документе n_doc\n",
        "                                                          ...,\n",
        "                                                          # сколько токенов в документе n_doc\n",
        "                                                          ... \n",
        "                                                          )\n",
        "\n",
        "            # сортируем по tf-idf\n",
        "            desc_tfidf_tokens = sorted(token_weights.items(), key=lambda x: (x[1], x[0]), reverse=True)\n",
        "            extracted_tfidf.append(desc_tfidf_tokens)\n",
        "            return extracted_tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим наш первый датасет"
      ],
      "metadata": {
        "id": "mIIKM8Wpoyaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"истинные\" наиболее представительные слова в первых 10 документах\n",
        "reference = [\n",
        "    \"cheaper supply orbits reach economies c5hcbo alike allen ground repairstation\",\n",
        "    \"reston overhead wrap allen leadership fee integration dennis centers nasa\",\n",
        "    \"revolt grasp alaska files acad3 prograsm geta cshow autocad 124722\",\n",
        "    \"list rankings krumenaker 71160 2356 source larry compuserve traffic unsubscribed\",\n",
        "    \"class foundation banquet teachers dinner teaching studies space lichtenberg planetary\",\n",
        "    \"access muc hicksville flaking expecting redundancy navstar bird pat digex\",\n",
        "    \"lehigh children abominable tfv0 ucdavis wealth starving capital games dan\",\n",
        "    \"processors silicon lower slower ssrt higher access future hjistorical germanium\",\n",
        "    \"wpi maverick giaquinto worcester novice 2280 01609 information shuttles periodicals\",\n",
        "    \"accelerations henry toronto acceleration andrew immersion generalizes endured efpx7ws00uh7qaop1s zoology\"\n",
        "]"
      ],
      "metadata": {
        "id": "dr6IZsIlTGsY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKED_DOCS = len(reference)\n",
        "CHECKED_TOP = len(reference[0].split(\" \"))"
      ],
      "metadata": {
        "id": "yxGj9fbyTHpf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKED_DOCS, CHECKED_TOP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjyAwRwHTNhI",
        "outputId": "28c84fb3-f44f-450b-aa15-245ec3c6d038"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# один из стандартных наборов данных для классификации текстов\n",
        "newsgroups = datasets.fetch_20newsgroups(subset='all', categories=['sci.space'])\n",
        "\n",
        "# тексты\n",
        "documents = newsgroups.data"
      ],
      "metadata": {
        "id": "lj9GPqvFTk-Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запустим подсчёт TFIDF сначала на униграммах"
      ],
      "metadata": {
        "id": "mnXQ5QNpn0OP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extractor = TfIdfExtractor()\n",
        "extractor.compute_counts_in_collection_collection(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "BWvZL6C8YGWK",
        "outputId": "e2cdb9e9-000a-4bb4-afc7-bfef29a3a3c5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1294b8cb3764>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mextractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfIdfExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_counts_in_collection_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-b2fce1fe8237>\u001b[0m in \u001b[0;36mcompute_counts_in_collection_collection\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mсырые\u001b[0m \u001b[0mтексты\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \"\"\"\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs_with_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_hits_count_in_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_count_in_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__count_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b2fce1fe8237>\u001b[0m in \u001b[0;36m__count_tokens\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mtoken_hits_count_in_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mtoken_count_in_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'append'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision_accumulator = 0.0\n",
        "extracted_tfidf = extractor.extract_tfidf(CHECKED_DOCS)\n",
        "\n",
        "# посчитаем топ по tf-idf для первых десяти документов\n",
        "for n_doc in range(CHECKED_DOCS):\n",
        "    # сортируем по tf-idf\n",
        "    desc_tfidf_tokens = extracted_tfidf[n_doc]\n",
        "    tfidf_top = list(map(lambda x: x[0], desc_tfidf_tokens))[:CHECKED_TOP]\n",
        "\n",
        "    # можно распечатать и посмотреть, отличается ли порядок (не должен)\n",
        "    # print(\" \".join(tfidf_top))\n",
        "    # print(reference[n_doc])\n",
        "\n",
        "    # вычисляем что-то вроде Precision@10\n",
        "    tfidf_top_set = set(tfidf_top)\n",
        "    in_reference = len(tfidf_top_set.intersection(set(reference[n_doc].split(\" \"))))\n",
        "    precision = in_reference / CHECKED_TOP\n",
        "\n",
        "    print(n_doc, \"Precision: %.1f%%\" % (precision * 100.0))\n",
        "    precision_accumulator += precision\n",
        "\n",
        "# необходимое, но не достаточное условие: у правильного решения здесь должно быть ровно 100%\n",
        "print(\"Avg precision: %.2f%%\" % (float(precision_accumulator) / CHECKED_DOCS * 100.0))"
      ],
      "metadata": {
        "id": "SEVaD6CbMwr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь прогоним наш подсчёт на биграммах"
      ],
      "metadata": {
        "id": "PHWgexg0oId2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"истинные\" наиболее представительные словосочетания в первых 10 документах\n",
        "reference = [\n",
        "    'satellite_orbit supply_make supply_facility sources_cheaper source_supply scale_allen repair_supply remember_presence ready_source reached_ready',\n",
        "    'large_scale wrap_overhead work_it stated_wrap lack_leadership wrong_nasa successful_large single_important scale_effort reston_nasa',\n",
        "    'alaska_edu acad3_alaska think_cshow space_design slide_shows shuttle_design shows_think revolt_look revolt_if related_items',\n",
        "    'larry_krumenaker 71160_2356 2356_compuserve compuserve_com won_answer unsubscribed_list unfortunately_did traffic_rankings traffic_given temporarily_unsubscribed',\n",
        "    'studies_foundation planetary_studies dinner_banquet the_class teaching_newest space_teaching newest_frontier hours_graduate graduate_credit foundation_the',\n",
        "    'users_areas soon_orbit redundancy_plane provide_redundancy orbit_major orbit_hicksville net_bird needed_provide muc_user major_users',\n",
        "    'lehigh_edu ucdavis_edu workers_end wo_man wealth_way way_ist viewpoint_subject tv_companies trying_everyone thats_abominable',\n",
        "    'computer_technology what_argument weight_ssrt times_lower throw_hjistorical this_kind think_dc theoretically_future technology_stated technology_actually',\n",
        "    'wpi_wpi wpi_edu maverick_wpi space_program worcester_ma todd_giaquinto suggest_books subject_general sites_novice shuttles_history',\n",
        "    'andrew_cmu water_immersion violent_deceleration using_water think_30 talking_sustained sustained_acceleration still_higher sounds_bit odd_gees'\n",
        "]"
      ],
      "metadata": {
        "id": "pFvyreZ-okXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_extractor = TfIdfExtractor(ngrams=2)\n",
        "bigram_extractor.compute_counts_in_collection_collection(documents)"
      ],
      "metadata": {
        "id": "_Mile9HZX7pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_accumulator = 0.0\n",
        "extracted_tfidf = bigram_extractor.extract_tfidf(CHECKED_DOCS)\n",
        "\n",
        "for n_doc in range(CHECKED_DOCS):\n",
        "    desc_tfidf_tokens = extracted_tfidf[n_doc]\n",
        "    tfidf_top = list(map(lambda x: x[0], desc_tfidf_tokens))[:CHECKED_TOP]\n",
        "\n",
        "    tfidf_top_set = set(tfidf_top)\n",
        "    in_reference = len(tfidf_top_set.intersection(set(reference[n_doc].split(\" \"))))\n",
        "    precision = in_reference / CHECKED_TOP\n",
        "\n",
        "    print(n_doc, \"Precision: %.1f%%\" % (precision * 100.0))\n",
        "    precision_accumulator += precision\n",
        "\n",
        "print(\"Avg precision: %.2f%%\" % (float(precision_accumulator) / CHECKED_DOCS * 100.0))"
      ],
      "metadata": {
        "id": "TDtnhQiyX7j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "А теперь попробуйте посчитать tf-idf на любом значении параметра `ngrams` "
      ],
      "metadata": {
        "id": "PEeN8-zDXdHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_extractor = TfIdfExtractor(ngrams=?)\n",
        "ngram_extractor.compute_counts_in_collection_collection(documents)"
      ],
      "metadata": {
        "id": "N-7EDe_cX7JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_accumulator = 0.0\n",
        "extracted_tfidf = bigram_extractor.extract_tfidf(CHECKED_DOCS)\n",
        "\n",
        "for n_doc in range(CHECKED_DOCS):\n",
        "    desc_tfidf_tokens = extracted_tfidf[n_doc]\n",
        "    tfidf_top = list(map(lambda x: x[0], desc_tfidf_tokens))[:CHECKED_TOP]\n",
        "    print(f'TOP-{CHECKED_TOP} important words for {n_doc}th document: {tfidf_top}')"
      ],
      "metadata": {
        "id": "tJrdiybGX7fh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}